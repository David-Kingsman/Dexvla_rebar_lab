import torch

def quick_test():
    print("=== å¿«é€ŸFlash Attentionæµ‹è¯• ===")
    
    # æ£€æŸ¥åŸºç¡€ç¯å¢ƒ
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
    
    # æµ‹è¯•Flash Attentionï¼ˆä¸å¯¼å…¥transformersï¼‰
    try:
        import flash_attn
        from flash_attn import flash_attn_func
        print("âœ… Flash Attentionå¯ä»¥å¯¼å…¥")
        
        # ç®€å•è®¡ç®—æµ‹è¯•
        if torch.cuda.is_available():
            q = torch.randn(1, 10, 4, 16, dtype=torch.float16, device='cuda')
            k = torch.randn(1, 10, 4, 16, dtype=torch.float16, device='cuda')
            v = torch.randn(1, 10, 4, 16, dtype=torch.float16, device='cuda')
            out = flash_attn_func(q, k, v)
            print("âœ… Flash Attentionè®¡ç®—æˆåŠŸ")
            return True
    except Exception as e:
        print(f"âŒ Flash Attentionä¸å¯ç”¨: {e}")
        return False

if __name__ == "__main__":
    result = quick_test()
    print("\n" + "="*40)
    if result:
        print("ğŸ‰ å»ºè®®ä½¿ç”¨: --flash_attn True")
    else:
        print("âŒ å¿…é¡»ä½¿ç”¨: --flash_attn False")